{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ww_ZBkRuyGJ"
   },
   "source": [
    "# SECTION 1: Introduction\n",
    "\n",
    "Welcome to my NLP project using GPT-3 and Seinfeld data. The goal is to allow the user to prompt GPT-3 with something and have it respond with an AI-generated Seinfeld situation.\n",
    "\n",
    "Example:\n",
    "> Prompt: \"Trying to connect to WiFi\"  \n",
    "> Response: \"When the WiFi George usually steals suddenly has a password, he becomes addicted to trying to \"hack\" in. J: 'Just get your own!' G: 'NEVER' \"\n",
    "\n",
    "(The response is one of the examples from the @ModernSeinfeld twitter feed.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmRjTL4myYC2"
   },
   "source": [
    "\n",
    "# SECTION 2: Data\n",
    "\n",
    "The data used is:\n",
    "\n",
    "\n",
    "*   Seinfeld episodes synopsis (173), from imdb and scraped here: https://www.kaggle.com/bcruise/seinfeld-episodes\n",
    "*   @ModernSeinfeld tweets (492), scraped using twint in the acompanying notebook\n",
    "*   Curb Your Enthusiasm episode synopsis, might be interesting to add later for more of a \"Larry David\" bot\n",
    "\n",
    "With a combined 565 examples, we should have enough data to fine-tune GPT-3. According to the OpenAI guide <https://beta.openai.com/docs/guides/fine-tuning>, \"we recommend having at least a couple hundred examples. In general, we've found that each doubling of the dataset size leads to a linear increase in model quality.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our episode dataframe contains synopses for 173 episodes. The first few examples are:\n",
      "0                                                                               Jerry and George argue whether an overnight visitor Jerry is expecting is coming with romantic intentions.\n",
      "1                                           Jerry and George stake out the lobby of an office building to find a woman Jerry met at a party but whose name and phone number he didn't get.\n",
      "2    After Jerry's apartment is robbed, Jerry starts to look for other apartments. But Jerry and George both want the same apartment, and Elaine wants the apartment of whoever loses out.\n",
      "Name: desc, dtype: object \n",
      "\n",
      "Our teets dataframe contains 492 tweets. The first few examples are: \n",
      "                                                                                                                                          tweet\n",
      "0  George's GF wants a \"no phones at dinner\" rule. G: \"We had a good thing going, Jerry!  Now we're supposed to talk? That can only end badly!\"\n",
      "1   After her fuck buddy texts her that she should come over to \"watch Netflix,\" Elaine is pissed when he actually just wants to watch Netflix.\n",
      "2   Jerry refuses to go to a Cash Only diner. J:\"They’ve seen the credit card! They know the credit card! It’s time to accept the credit card!\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#As we load the IMDB data, we narrow the columns we're looking for\n",
    "episodes_df = pd.read_csv('./data/seinfeld_imdb.csv.xls',usecols=['title','desc'])\n",
    "#In the tweets we also get rid of the first row, which is a tweet promoting the author's book\n",
    "tweets_df = pd.read_csv('./data/SeinfeldToday_tweets.csv',usecols=['tweet'], skiprows=[1])\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(f\"Our episode dataframe contains synopses for {len(episodes_df)} episodes. The first few examples are:\\n{episodes_df['desc'].head(3)} \\n\")\n",
    "print(f\"Our teets dataframe contains {len(tweets_df)} tweets. The first few examples are: \\n{tweets_df.head(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning tasks:\n",
    "* Strip leading whitespace, especially from episode data\n",
    "* In tweets, convert quotes from J:\"They've seen the credit card!\" to Jerry:\"They've seen the credit card!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the pre-training file\n",
    "\n",
    "Your data must be a JSONL document, where each line is a prompt-completion pair corresponding to a training example. The format looks like:\n",
    "  > {\"prompt\": \"\\<prompt text\\>\", \"completion\": \"\\<ideal generated text\\>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to help format each example\n",
    "def new_example(completion: str, prompt: str=None) -> dict:\n",
    "    \"\"\"\\\n",
    "    Formats a single traning example for GPT-3 and return a dict.\n",
    "    \n",
    "    Args:\n",
    "        completion (str): The desired completion of the example\n",
    "        prompt (str): optional, the prompt part of the example \n",
    "            (default is None)\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with prompt and completion keys. A str() representation of this dict will meet the desired GPT-3 format.\n",
    "    \n",
    "    Your data must be a JSONL document, where each line is a prompt-completion pair corresponding to a training example. The format looks like:\n",
    "    {\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "    \n",
    "    - Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is \\\\n\\\\n###\\\\n\\\\n. The separator should not appear elsewhere in any prompt.\n",
    "    - Each completion should start with a whitespace due to our tokenization, which tokenizes most words with a preceding whitespace.\n",
    "    - Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be \\\\n, ###, or any other token that does not appear in any completion.\n",
    "    - For inference, you should format your prompts in the same way as you did when creating the training dataset, including the same separator. Also specify the same stop sequence to properly truncate the completion.\n",
    "    \"\"\"\n",
    "    \n",
    "    example = {}\n",
    "    example = {\"prompt\": str(prompt), \"completion\": completion}\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'None', 'completion': 'test'}\n"
     ]
    }
   ],
   "source": [
    "print(new_example('test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 3: Working with Open AI and GPT3\n",
    "\n",
    "- First we load the API Key\n",
    "- Then we fine-tune a model, following https://beta.openai.com/docs/guides/fine-tuning\n",
    "- Then we test prompting the model with something and seeing what Seinfeldy situation it comes up with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "83NW3EbHyblr",
    "outputId": "0a5135a5-1896-483c-b8ab-b650cfb1eca2"
   },
   "outputs": [],
   "source": [
    "!pip -q install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fMBfq3k6ywa4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYHnb3_QyzYg",
    "outputId": "d9ceb7c1-dc9e-414f-fa05-f0a46ad72c1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/dan/dev/miniconda3/lib/python3.9/site-packages (0.19.2)\r\n"
     ]
    }
   ],
   "source": [
    "# Load OpenAI API Key\n",
    "\n",
    "try:\n",
    "  # When in Colab\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  with open(\"/content/drive/My Drive/Colab Notebooks/GPT3_api\", 'r') as file:\n",
    "    openai.api_key = file.read().rstrip('\\n')\n",
    "except:\n",
    "  # When in local dev environment\n",
    "  try:\n",
    "     # Load variables from .env file in working directory\n",
    "     !pip install python-dotenv\n",
    "     from dotenv import load_dotenv\n",
    "     load_dotenv()\n",
    "  except:\n",
    "     # You'll need to set the environment variables somehow, perhaps in .bashrc\n",
    "     print(\"Warning: .env file not found\")\n",
    "  API_KEY = os.getenv('PROJECT_API_KEY')\n",
    "  openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R55jALEc9IiB",
    "outputId": "ccd142cb-59a9-4d81-c342-23eaa26d9ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\n\\nThis is a test\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1644344165,\n",
      "  \"id\": \"cmpl-4ZPK1Y6D34H8YooQRTdNznDCfDhvS\",\n",
      "  \"model\": \"text-davinci:001\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test OpenAI API\n",
    "response = openai.Completion.create(engine=\"text-davinci-001\", prompt=\"Say this is a test\", max_tokens=6)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlQiyIz4NR7C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMLfP4YFWnKfytVMOq0uUOc",
   "mount_file_id": "16xcsyKi8vCBHGntbHljNFp77ranRAG83",
   "name": "GPT3 Seinfeld.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
