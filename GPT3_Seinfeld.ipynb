{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ww_ZBkRuyGJ"
   },
   "source": [
    "# SECTION 1: Introduction\n",
    "\n",
    "Welcome to my NLP project using GPT-3 and Seinfeld data. The goal is to allow the user to prompt GPT-3 with something and have it respond with an AI-generated Seinfeld situation.\n",
    "\n",
    "Example:\n",
    "> Prompt: \"Trying to connect to WiFi\"  \n",
    "> Response: \"When the WiFi George usually steals suddenly has a password, he becomes addicted to trying to \"hack\" in. J: 'Just get your own!' G: 'NEVER' \"\n",
    "\n",
    "(The response is one of the examples from the @ModernSeinfeld twitter feed.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmRjTL4myYC2"
   },
   "source": [
    "\n",
    "# SECTION 2: Data\n",
    "\n",
    "The data used is:\n",
    "\n",
    "\n",
    "*   Seinfeld episodes synopsis (173), from imdb and scraped here: https://www.kaggle.com/bcruise/seinfeld-episodes\n",
    "*   @ModernSeinfeld tweets (492), scraped using twint in the acompanying notebook\n",
    "*   Curb Your Enthusiasm episode synopsis, might be interesting to add later for more of a \"Larry David\" bot\n",
    "\n",
    "That makes for a total of 665 examples. According to the OpenAI guide <https://beta.openai.com/docs/guides/fine-tuning>, \"we recommend having at least a couple hundred examples. In general, we've found that each doubling of the dataset size leads to a linear increase in model quality.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our episode dataframe contains synopses for 173 episodes. The first few examples are:\n",
      "0                                                                               Jerry and George argue whether an overnight visitor Jerry is expecting is coming with romantic intentions.\n",
      "1                                           Jerry and George stake out the lobby of an office building to find a woman Jerry met at a party but whose name and phone number he didn't get.\n",
      "2    After Jerry's apartment is robbed, Jerry starts to look for other apartments. But Jerry and George both want the same apartment, and Elaine wants the apartment of whoever loses out.\n",
      "Name: desc, dtype: object \n",
      "\n",
      "Our teets dataframe contains 492 tweets. The first few examples are: \n",
      "                                                                                                                                          tweet\n",
      "0  George's GF wants a \"no phones at dinner\" rule. G: \"We had a good thing going, Jerry!  Now we're supposed to talk? That can only end badly!\"\n",
      "1   After her fuck buddy texts her that she should come over to \"watch Netflix,\" Elaine is pissed when he actually just wants to watch Netflix.\n",
      "2   Jerry refuses to go to a Cash Only diner. J:\"They’ve seen the credit card! They know the credit card! It’s time to accept the credit card!\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#As we load the IMDB data, we narrow the columns we're looking for\n",
    "episodes_df = pd.read_csv('./data/seinfeld_imdb.csv.xls',usecols=['title','desc'])\n",
    "#In the tweets we also get rid of the first row, which is a tweet promoting the author's book\n",
    "tweets_df = pd.read_csv('./data/SeinfeldToday_tweets.csv',usecols=['tweet'], skiprows=[1])\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(f\"Our episode dataframe contains synopses for {len(episodes_df)} episodes. The first few examples are:\\n{episodes_df['desc'].head(3)} \\n\")\n",
    "print(f\"Our teets dataframe contains {len(tweets_df)} tweets. The first few examples are: \\n{tweets_df.head(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning tasks:\n",
    "* Strip leading and trailing whitespace, especially from episode data\n",
    "* In tweets, convert quotes from J:\"They've seen the credit card!\" to Jerry:\"They've seen the credit card!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the tweets have quotes, such as the first example does, this replaces the single-letter character with the fullname\n",
    "# (i.e. Replaces G: with George:) \n",
    "def repl(match):\n",
    "    x = match.group(1)\n",
    "    return {\n",
    "        'G': \"George\",\n",
    "        'J': \"Jerry\",\n",
    "        'E': \"Elaine\",\n",
    "        'K': \"Kramer\"\n",
    "    }[x] + \": \"\n",
    "\n",
    "tweets_df['tweet'] = tweets_df['tweet'].str.replace(r'([JGEK])(:\\s*\"?)', repl, regex=True).str.strip()\n",
    "episodes_df['desc'] = episodes_df['desc'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual review of examples\n",
    "Since there aren't that many examples, we can take a look at all of them.\n",
    "Using a simple data labeling python extension called Pigeon, we can mark each example as 'ready' or 'needs cleaning'. While we review we can also chuckle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip -qq install pigeon-jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662666db9bb9419ebfd645644d47fc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='0 examples annotated, 666 examples left')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519452445e2b4a8d8a4a3ed6c9dedc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='ready', style=ButtonStyle()), Button(description='needs cleaning', style=Bu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39d2e4a87564fe8b3db5091c01e9441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pigeon import annotate\n",
    "annotations = annotate(\n",
    "  (*tweets_df['tweet'], *episodes_df['desc']),\n",
    "  options=['ready', 'needs cleaning']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for GPT3 fine-tuning\n",
    "\n",
    "GPT-3 allows for fine-tuning https://beta.openai.com/docs/guides/fine-tuning/\n",
    "\n",
    "### What is fine-tuning?\n",
    "\n",
    "> GPT-3 has been pre-trained on a vast amount of text from the open internet. When given a prompt with just a few examples, it can often intuit what task you are trying to perform and generate a plausible completion. This is often called \"few-shot learning.\"\n",
    ">\n",
    "> Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide examples in the prompt anymore.\n",
    "\n",
    "You fine-tune by providing data in a JSONL document, where each line is a prompt-completion pair corresponding to a training example. The format looks like:\n",
    "  > {\"prompt\": \"\\<prompt text\\>\", \"completion\": \"\\<ideal generated text\\>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to help format each example\n",
    "STOP_TOKEN = \" ##END##\"\n",
    "def new_example(completion: str, prompt: str='') -> str:\n",
    "    \"\"\"\\\n",
    "    Formats a single traning example for GPT-3 and return a string.\n",
    "    \n",
    "    Args:\n",
    "        completion (str): The desired completion of the example\n",
    "        prompt (str): optional, the prompt part of the example \n",
    "            (default is None)\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with prompt and completion keys. A str() representation of this dict will meet the desired GPT-3 format.\n",
    "    \n",
    "    Your data must be a JSONL document, where each line is a prompt-completion pair corresponding to a training example. The format looks like:\n",
    "    {\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "    \n",
    "    - Each prompt should end with a fixed separator to inform the model when the prompt ends and the completion begins. A simple separator which generally works well is \\\\n\\\\n###\\\\n\\\\n. The separator should not appear elsewhere in any prompt.\n",
    "    - Each completion should start with a whitespace due to our tokenization, which tokenizes most words with a preceding whitespace.\n",
    "    - Each completion should end with a fixed stop sequence to inform the model when the completion ends. A stop sequence could be \\\\n, ###, or any other token that does not appear in any completion.\n",
    "    - For inference, you should format your prompts in the same way as you did when creating the training dataset, including the same separator. Also specify the same stop sequence to properly truncate the completion.\n",
    "    \"\"\"\n",
    "    \n",
    "    import json\n",
    "    example = {}\n",
    "    example = {\"prompt\": str(prompt), \"completion\": \" \" + completion + STOP_TOKEN}\n",
    "    # Converting the dictionary to a string should be done with json.dumps otherwise it will have single quotes instead of double quotes\n",
    "    return json.dumps(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prompt\": \"\", \"completion\": \" test ##END##\"}\n"
     ]
    }
   ],
   "source": [
    "print(new_example('test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning for Contextual vs. Open-ended Generation\n",
    "\n",
    "For the examples we feed GPT-3, there are several options for what to put in the prompt and completion, depending on the use-case. The GPT-3 fine-tuning guide covers classification (e.g. sentiment analysis, email triage), contextual generation (e.g. write an advert based on wikipedia entry, product description based on technical properties), and open-ended generation (e.g maintaining company voice, generating haikus). https://beta.openai.com/docs/guides/fine-tuning/specific-guidelines\n",
    "\n",
    "To use contextual generation, which requires fewer examples for good performance, we would need to associate a prompt with each example. For instance, in the example at the beginning of this notebook we would perhaps feed the following line to GPT-3:\n",
    "\n",
    "  > {\"prompt\": \"Wifi\", \"completion\": \"When the WiFi George usually steals suddenly has a password, he becomes addicted to trying to \"hack\" in. J: 'Just get your own!' G: 'NEVER'\"}\n",
    "\n",
    "We might return to contextual generation, but it's hard to imagine how to do so for our data. Particularly with the episode synopses, how would we pick the prompts that entail the several plot-lines happening in each episode? And if we do provide prompts, would the model be able to generalize to new real-world situations that the user might throw out? \n",
    "\n",
    "Lets try first with open-ended fine-tuning. The guidelines are:\n",
    "\n",
    ">- Leave the prompt empty\n",
    ">- No need for any separators\n",
    ">- You'll normally want a very large number of examples, at least a few thousand\n",
    ">- Ensure the examples cover the intended domain or the desired tone of voice\n",
    ">- Example: {\"prompt\":\"\", \"completion\":\" <company voice textual content>\"}\n",
    "\n",
    "Note, that the documentation cautions open-ended genereation requires \"a few thousand examples\". Note also, \"generative tasks have a potential to leak training data when requesting completions from the model.\" With relatively few examples (655), the model might just parrot back some of the data we feed it regardless of the situation we prompt it with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the file with examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"prompt\": \"\", \"completion\": \" After her fuck buddy texts her that she should come over to \\\\\"watch Netflix,\\\\\" Elaine is pissed when he actually just wants to watch Netflix. ##END##\"}', '{\"prompt\": \"\", \"completion\": \" Jerry refuses to go to a Cash Only diner. Jerry: They\\\\u2019ve seen the credit card! They know the credit card! It\\\\u2019s time to accept the credit card!\\\\\" ##END##\"}'] \n",
      "...\n",
      " ['{\"prompt\": \"\", \"completion\": \" Jerry, George, Kramer and Elaine get stuck in standstill traffic due to the massive Puerto Rican Day Parade. ##END##\"}', '{\"prompt\": \"\", \"completion\": \" Just as the four are about to go to the movies, Jerry looks back on the past nine years with the audience. ##END##\"}', '{\"prompt\": \"\", \"completion\": \" After George and Jerry land a production deal with NBC, the four head out for Paris on NBC\\'s private plane and are waylaid in a small Massachusetts town. ##END##\"}']\n"
     ]
    }
   ],
   "source": [
    "TRAIN_FILE = \"./data/training_examples.JSONL\"\n",
    "jsonl = []\n",
    "\n",
    "for example in (*tweets_df['tweet'], *episodes_df['desc']):\n",
    "   jsonl.append(new_example(example))\n",
    "#print the first and last 3 items in the JSONL\n",
    "print(JSONL[1:3], \"\\n...\\n\", JSONL[-3:])\n",
    "\n",
    "with open(TRAIN_FILE, \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(str(item) for item in JSONL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 3: Using the Open AI and GPT3 APIs\n",
    "\n",
    "- Use OpenAI command-line tools to validate our training file\n",
    "- Load the API Key\n",
    "- Fine-tune a model, following https://beta.openai.com/docs/guides/fine-tuning\n",
    "- Prompting the model with something and seeing what Seinfeldy situation it comes up with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "83NW3EbHyblr",
    "outputId": "0a5135a5-1896-483c-b8ab-b650cfb1eca2"
   },
   "outputs": [],
   "source": [
    "!pip -qq install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging requires wandb to be installed. Run `pip install wandb`.\n",
      "Analyzing...\n",
      "\n",
      "- Your file contains 665 prompt-completion pairs\n",
      "\n",
      "No remediations found.\n",
      "\n",
      "You can use your file for fine-tuning:\n",
      "> openai api fine_tunes.create -t \"./data/training_examples.JSONL\"\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\" ##END##\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 11.58 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "# Test the training file to \n",
    "!openai tools fine_tunes.prepare_data -f {TRAIN_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYHnb3_QyzYg",
    "outputId": "d9ceb7c1-dc9e-414f-fa05-f0a46ad72c1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/dan/dev/miniconda3/lib/python3.9/site-packages (0.19.2)\r\n"
     ]
    }
   ],
   "source": [
    "# Load OpenAI API Key\n",
    "import os\n",
    "import openai\n",
    "\n",
    "try:\n",
    "  # When in Colab\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  with open(\"/content/drive/My Drive/Colab Notebooks/GPT3_api\", 'r') as file:\n",
    "    openai.api_key = file.read().rstrip('\\n')\n",
    "except:\n",
    "  # When in local dev environment\n",
    "  try:\n",
    "     # Load variables from .env file in working directory\n",
    "     !pip install python-dotenv\n",
    "     from dotenv import load_dotenv\n",
    "     load_dotenv()\n",
    "  except:\n",
    "     # You'll need to set the environment variables somehow, perhaps in .bashrc\n",
    "     print(\"Warning: .env file not found\")\n",
    "  API_KEY = os.getenv('PROJECT_API_KEY')\n",
    "  openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R55jALEc9IiB",
    "outputId": "ccd142cb-59a9-4d81-c342-23eaa26d9ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\n\\nThis is a test\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1644344165,\n",
      "  \"id\": \"cmpl-4ZPK1Y6D34H8YooQRTdNznDCfDhvS\",\n",
      "  \"model\": \"text-davinci:001\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test OpenAI API\n",
    "response = openai.Completion.create(engine=\"text-davinci-001\", prompt=\"Say this is a test\", max_tokens=6)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlQiyIz4NR7C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMLfP4YFWnKfytVMOq0uUOc",
   "mount_file_id": "16xcsyKi8vCBHGntbHljNFp77ranRAG83",
   "name": "GPT3 Seinfeld.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
